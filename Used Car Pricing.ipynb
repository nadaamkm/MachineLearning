{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "In this application, you will explore a dataset from kaggle that contains information on 3 million used cars.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To frame the task, throughout our practical applications we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal of this project is to understand the history of sales for used cars and predict the pricing of cars.\n",
    "- The data will need to be defined, manipulated and cleaned to start building the model.\n",
    "- The model should deliver predict the future consumer pricing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/vehicles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.select_dtypes(include=object):\n",
    "    if data[column].nunique() < 11:\n",
    "        sns.countplot(y=column, data=data)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A few notes about the data\n",
    "There aren't a lot of data for hybrid cars, new & fair conditions, 3 5 and 10 cylinder cars.\n",
    "The title column won't tell us too much about the car since most of the data is in clean.\n",
    "Most cars are in automatic transmission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.corr()\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can't really see any high correlation here. Let's plot all the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "for column in data:\n",
    "        fig=px.histogram(data, x=column)\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode to change it from object to integer\n",
    "label_encoder= preprocessing.LabelEncoder()\n",
    "data_encoded= data\n",
    "for column in data_encoded.select_dtypes(include=object):\n",
    "    data_encoded[column]=label_encoder.fit_transform(data_encoded[column])\n",
    "    \n",
    "data_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data:\n",
    "    print(\"Unique # of\",column ,data[column].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "After our initial exploration and fine tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes about the data\n",
    "\n",
    "- I will remove ID and VIN number from the dataset because it is not necessary for modeling. Also, most of the size column is missing so that will likely be dropped as well. I will also drop model column, there are too many unique values (29650) to make a concrete observation.\n",
    "- Remove null values\n",
    "- There are no duplicate entries\n",
    "- There are currently 4 numeric features and I intend to change year and odometer to integers. \n",
    "- Remove outliers. As seen from the histograms the price, odometer and years have a very large range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data=data.drop([\"VIN\", \"id\", \"size\", \"model\"], axis=1, inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the null values in year and odometer columns \n",
    "# remember the inplace=true removes the existing dataframe\n",
    "car_data=car_data.dropna(subset=['year', 'odometer']).reset_index(drop=True)\n",
    "car_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data=car_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change year and price to integer\n",
    "car_data.year=car_data.year.astype(int)\n",
    "car_data.price=car_data.price.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-score is a method for removing outliers (also called standard score). This value helps to understand how far the data point from the mean. \n",
    "\n",
    "Z-score=(data_point - mean)/(Std. deviation)\n",
    "\n",
    "However, in this case I want to use IQR (Inter Quartile Range). \n",
    "IQR= Quartile3-Quartile1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers=['price', 'odometer', 'year']\n",
    "for column in outliers:\n",
    "    Q1= np.percentile(car_data[column], 25, interpolation='midpoint')\n",
    "    Q3=np.percentile(car_data[column], 75, interpolation='midpoint')\n",
    "    IQR=Q3-Q1\n",
    "    #To define the outlier base valye is defined above and below \n",
    "    #datasets normal range (namely upper and lower bounds), define \n",
    "    #the upper and lower bound\n",
    "    upper= (Q3+(1.5*IQR))\n",
    "    lower=(Q1- (1.5*IQR))\n",
    "    \n",
    "    car_data.loc[car_data[column] < lower] =np.nan\n",
    "    car_data.loc[car_data[column] > upper] =np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(car_data.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=car_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring a number of different models, let's start with linear regression\n",
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encoding for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(car_data['odometer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data=car_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.odometer=car_data.odometer.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "transformer = TransformedTargetRegressor(regressor = Ridge())\n",
    "transformer= TransformedTargetRegressor(regressor=Ridge())\n",
    "pipe1= Pipeline([(\"c_transform\", ce.TargetEncoder(cols=categories)),\n",
    "                ('scaler', StandardScaler())])\n",
    "pipe1.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv # For dataframes\n",
    "from pandas import DataFrame # For dataframes\n",
    "from numpy import ravel # For matrices\n",
    "import matplotlib.pyplot as plt # For plotting data\n",
    "import seaborn as sns # For plotting data\n",
    "from sklearn.model_selection import train_test_split # For train/test splits\n",
    "from sklearn.neighbors import KNeighborsClassifier # The k-nearest neighbor classifier\n",
    "from sklearn.feature_selection import VarianceThreshold # Feature selector\n",
    "from sklearn.pipeline import Pipeline # For setting up pipeline\n",
    "# Various pre-processing steps\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, PowerTransformer, MaxAbsScaler, LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_odometer_train, X_odometer_test, y_odometer_train, y_odometer_test=train_test_split(car_data[['odometer']],car_data['price'], test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "transformer = TransformedTargetRegressor(regressor = Ridge())\n",
    "pipe1= Pipeline([('scaler', StandardScaler()), ('ridge', Ridge())])\n",
    "#X_odometer= car_data[['odometer']]\n",
    "#y_odometer=car_data['price']\n",
    "pipe1.fit(X_odometer_train, y_odometer_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set score: ' + str(pipe1.score(X_odometer_train,y_odometer_train)))\n",
    "print('Test set score: ' + str(pipe1.score(X_odometer_test,y_odometer_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {'ridge__alpha': [0.001, 0.1, 1.0, 10.0, 100.0, 1000.0]}\n",
    "ridge=Ridge()\n",
    "grid = GridSearchCV(pipe1, param_grid=param_dict)\n",
    "grid.fit(X_odometer_train, y_odometer_train)\n",
    "\n",
    "train_preds=grid.predict(X_odometer_train)\n",
    "test_preds= grid.predict(X_odometer_test)\n",
    "model_2_train_mse = mean_squared_error(y_odometer_train, train_preds)\n",
    "model_2_test_mse = mean_squared_error(y_odometer_test, test_preds)\n",
    "model_2_best_alpha = grid.best_params_\n",
    "\n",
    "print(f'Test MSE: {model_2_test_mse}')\n",
    "print(f'Best Alpha: {list(model_2_best_alpha.values())[0]}')\n",
    "print('Training set score: ' + str(grid.score(X_odometer_train, y_odometer_train)))\n",
    "print('Test set score: ' + str(grid.score(X_odometer_test, y_odometer_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model just the numerical data \n",
    "The purpose of this is to explore building models with numerical data only. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Set up a pipeline using the Pipeline object from sklearn.pipeline\n",
    "\n",
    "Pipelines are used to automate a machine learning workflow. The pipeline can involve pre-processing, feature selection, classification/ regression and post-processing.\n",
    "\n",
    "2. Perform a grid search for the best parameters using GridSearchCV()\n",
    "\n",
    "Optimization tunes the model for the best performance. The success of any learning model rests on the selection of the best parameters that give the best possible results. \n",
    "\n",
    "3. Analyze the results and visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaler: For pre-processing data, i.e., transform the data to zero mean and unit variance using the StandardScaler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num=car_data[['odometer', 'year']]\n",
    "y_num=car_data['price']\n",
    "X_num_train, X_num_test, y_num_train, y_num_test=train_test_split(X_num,y_num, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "transformer = TransformedTargetRegressor(regressor = Ridge())\n",
    "pipe1= Pipeline([('scaler', StandardScaler()), ('ridge', Ridge())])\n",
    "#X_odometer= car_data[['odometer']]\n",
    "#y_odometer=car_data['price']\n",
    "pipe1.fit(X_num_train, y_num_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set score: ' + str(pipe1.score(X_num_train,y_num_train)))\n",
    "print('Test set score: ' + str(pipe1.score(X_num_test,y_num_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {'ridge__alpha': [0.001, 0.1, 1.0, 10.0, 100.0, 1000.0]}\n",
    "ridge=Ridge()\n",
    "grid1 = GridSearchCV(pipe2, param_grid=param_dict)\n",
    "grid1.fit(X_num_train, y_num_train)\n",
    "\n",
    "train_preds1=grid1.predict(X_num_train)\n",
    "test_preds1= grid1.predict(X_num_test)\n",
    "model_1_train_mse = mean_squared_error(y_num_train, train_preds)\n",
    "model_1_test_mse = mean_squared_error(y_num_test, test_preds)\n",
    "model_1_best_alpha = grid1.best_params_\n",
    "\n",
    "print(f'Test MSE: {model_1_test_mse}')\n",
    "print(f'Best Alpha: {list(model_1_best_alpha.values())[0]}')\n",
    "print('Training set score: ' + str(grid1.score(X_num_train, y_num_train)))\n",
    "print('Test set score: ' + str(grid1.score(X_num_test, y_num_test)))\n",
    "print(f\"R2: {r2_score(y_test, test_preds1)*100:.4}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10), dpi =80)\n",
    "plt.scatter(x=y_num_test, y=test_preds, alpha=0.6)\n",
    "plt.plot([0, 60000], [0,60000], 'r-')\n",
    "plt.title(\"Actual Price vs. Predicted Price Model 1\")\n",
    "plt.xlabel(\"Actual Price\")\n",
    "plt.ylabel(\"Predicted Price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training score: how the model is fitted in the training data. \n",
    "\n",
    "Test score: The higher the score, the better the model is generalized. \n",
    "\n",
    "Very low training score and low test score is under-fitting. Which is expected here as we only use year and odometer to fit the data. This model was just generated for practice and representation.\n",
    "\n",
    "As we can see from the graph it does not fit well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try categorical modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target encoding replaces a categorical feature with average target value of all data points belonging to the category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a Pipeline in a GridSearchCV you want to preface the value in your parameter dictionary with an all lowercase version of the object. For example, to search over a ridge estimators alpha value we will create a pipeline with names scaler and ridge to use the StandardScaler followed by the Ridge regressor. To search over the ridge objects alpha paramater we write ridge__alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer, TransformedTargetRegressor\n",
    "\n",
    "categories =[]\n",
    "for column in car_data.select_dtypes(include=object):\n",
    "    categories.append(column)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= car_data.drop('price', axis=1)\n",
    "y=car_data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "X_train.shape\n",
    "#X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = Pipeline(\n",
    "    [\n",
    "        (\"column_transform\", ce.TargetEncoder(cols = categories)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('ridge', Ridge())\n",
    "    ])\n",
    "param_dict = {'ridge__alpha': [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2 = GridSearchCV(preprocessor, param_grid=param_dict)\n",
    "grid2.fit(X_train, y_train)\n",
    "\n",
    "train_preds2=grid2.predict(X_train)\n",
    "test_preds2= grid2.predict(X_test)\n",
    "model_2_train_mse = mean_squared_error(y_train, train_preds)\n",
    "model_2_test_mse = mean_squared_error(y_test, test_preds)\n",
    "model_2_best_alpha = grid2.best_params_\n",
    "\n",
    "print(f'Test MSE: {model_2_test_mse}')\n",
    "print(f'Best Alpha: {list(model_2_best_alpha.values())[0]}')\n",
    "print('Training set score: ' + str(grid2.score(X_train, y_train)))\n",
    "print('Test set score: ' + str(grid2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R2: {r2_score(y_test, test_preds2)*100:.4}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10), dpi =80)\n",
    "plt.scatter(x=y_test, y=test_preds2, alpha=0.6)\n",
    "plt.plot([0, 60000], [0,60000], 'r-')\n",
    "plt.title(\"Actual Price vs. Predicted Price Model 2\")\n",
    "plt.xlabel(\"Actual Price\")\n",
    "plt.ylabel(\"Predicted Price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeating the model above but a slight different method to remove warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1=train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "X_train.shape\n",
    "\n",
    "scaler = StandardScaler()\n",
    "transformer = TransformedTargetRegressor(regressor=Ridge())\n",
    "\n",
    "preprocessor1 = Pipeline(\n",
    "    [\n",
    "        (\"column_transform\", ce.TargetEncoder(cols = categories)),\n",
    "        ('scaler', StandardScaler()),\n",
    "    ])\n",
    "param_dict = {'regressor__alpha': [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid5 = GridSearchCV(transformer, param_grid=param_dict)\n",
    "model5=make_pipeline(preprocessor1, grid5)\n",
    "grid5=model5.fit(X_train1, y_train1)\n",
    "\n",
    "train_preds5=grid5.predict(X_train1)\n",
    "test_preds5= grid5.predict(X_test1)\n",
    "model_5_train_mse = mean_squared_error(y_train1, train_preds5)\n",
    "model_5_test_mse = mean_squared_error(y_test1, test_preds5)\n",
    "\n",
    "print(f'Test MSE: {model_5_test_mse}')\n",
    "print(f'Best Alpha: {list(model_5_best_alpha.values())[0]}')\n",
    "print('Training set score: ' + str(grid5.score(X_train1, y_train1)))\n",
    "print('Test set score: ' + str(grid5.score(X_test1, y_test1)))\n",
    "print(f\"R2: {r2_score(y_test1, test_preds5)*100:.4}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3\n",
    "\n",
    "    Let's try Target Encoding with Sequential Selection with Lasso, Scaling and ridge. We will also add scoring in Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "X_train2, X_test2, y_train2, y_test2=train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "log_transform = TransformedTargetRegressor(regressor = Ridge(), transformer = StandardScaler())\n",
    "\n",
    "seq_selector = Pipeline([('selector', SequentialFeatureSelector(Lasso(), n_features_to_select = 12))])\n",
    "\n",
    "preprocessor = Pipeline(\n",
    "    [(\"column_transform\", ce.TargetEncoder(cols = categories, verbose=3)),\n",
    "    ('scaler', StandardScaler())])\n",
    "param_dict = {'regressor__alpha': [ 0.0001, 0.001, 0.01, 0.1, 100, 1000, 10000] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid3 = GridSearchCV(log_transformer, param_grid=param_dict, cv=5,\n",
    "                    scoring=('r2', 'neg_mean_absolute_error'),\n",
    "                    refit='neg_mean_absolute_error')\n",
    "model3=make_pipeline(preprocessor1,seq_selector, grid3)\n",
    "grid3=model3.fit(X_train2, y_train2)\n",
    "\n",
    "train_preds3=grid3.predict(X_train2)\n",
    "test_preds3= grid3.predict(X_test2)\n",
    "model_3_train_mse = mean_squared_error(y_train2, train_preds3)\n",
    "model_3_test_mse = mean_squared_error(y_test2, test_preds3)\n",
    "#model_5_best_alpha = grid5.best_params_\n",
    "\n",
    "print(f'Test MSE: {model_3_test_mse}')\n",
    "print(f'Best Alpha: {list(model_3_best_alpha.values())[0]}')\n",
    "print('Training set score: ' + str(grid3.score(X_train2, y_train2)))\n",
    "print('Test set score: ' + str(grid5.score(X_test2, y_test2)))\n",
    "print(f\"R2: {r2_score(y_test2, test_preds3)*100:.4}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10), dpi =80)\n",
    "plt.scatter(x=y_test2, y=test_preds3, alpha=0.6)\n",
    "plt.plot([0, 60000], [0,60000], 'r-')\n",
    "plt.title(\"Actual Price vs. Predicted Price Model 3\")\n",
    "plt.xlabel(\"Actual Price\")\n",
    "plt.ylabel(\"Predicted Price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "With some modeling accomplished, we aim to reflect on what we identify as a high quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight on drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have built 3 models.\n",
    "\n",
    "The first was just for practice purposes using numerical data. As expected, the model performance was extremely low. With the following information:\n",
    "\n",
    "    Test MSE: 118805993.95177384\n",
    "    Best Alpha: 100.0\n",
    "    Training set score: 0.11296720989770215\n",
    "    Test set score: 0.12188239664954037\n",
    "    R2: 12.19%\n",
    "\n",
    "The second performed the best out of all 3 models. \n",
    "\n",
    "    Test MSE: 72032734.99578267\n",
    "    Best Alpha: 100\n",
    "    Training set score: 0.46374065904117034\n",
    "    Test set score: 0.4657737480390959\n",
    "    R2: 46.58%\n",
    "\n",
    "The last performed similar to the second model, although slightly worse.\n",
    "\n",
    "    Test MSE: 72108690.87875488\n",
    "    Best Alpha: 1000.0\n",
    "    Training set score: -6213.154450031672\n",
    "    Test set score: 0.4659167625838996\n",
    "    R2: 46.52%\n",
    "    \n",
    "These scores could be better, initially I was aiming for an 80% R2 score. Given more time I would go back to the data exploration phase and clean the data further. I would also try another model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine tuning their inventory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can develop an application for the client where they can assess a price of the car. Given the larger error, they can record the difference between the model prediction and sales price and we can further improve the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
